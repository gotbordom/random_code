{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter notebooks\n",
    "\n",
    "This is a [Jupyter](http://jupyter.org/) notebook using Python.  You can install Jupyter locally to edit and interact with this notebook.\n",
    "\n",
    "# Rootfinding\n",
    "\n",
    "Rootfinding is the process of solving $$f(x) = 0$$ for $x$.  The standard assumption is that $f : R \\to R$ is _continuous_.  We are interested in developing general-purpose algorithms---those that can use $f(x)$ as a black box without needing to look inside.  When we implement our rootfinding algorithm in software, the user will pass a function or program to compute $f(x)$.  Rootfinding methods for differentiable functions may also use the derivative $f'(x)$.\n",
    "\n",
    "Some questions immediately arise:\n",
    "* **Existence.** When does this equation have at least one solution?\n",
    "* **Uniqueness.** When is the solution unique?\n",
    "\n",
    "Let's consider some test functions, defined here along with their derivatives which we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "pyplot.style.use('ggplot')\n",
    "import numpy\n",
    "\n",
    "tests = []\n",
    "\n",
    "@tests.append\n",
    "def f0(x):\n",
    "    return x*x - 2, 2*x\n",
    "\n",
    "@tests.append\n",
    "def f1(x):\n",
    "    return numpy.cos(x) - x, -numpy.sin(x) - 1\n",
    "\n",
    "#@tests.append\n",
    "def f2(x):\n",
    "    return numpy.exp(-numpy.abs(x)) + numpy.sin(x), numpy.exp(-numpy.abs(x))*(-numpy.sign(x)) + numpy.cos(x)\n",
    "\n",
    "@tests.append\n",
    "def f3(x):\n",
    "    return x*x - x + 0.25, 2*x - 1\n",
    "\n",
    "x = numpy.linspace(-2,2,100)\n",
    "pyplot.plot(x, 0*x, color='k')\n",
    "for f in tests:\n",
    "    pyplot.plot(x, f(x)[0], label=f.__name__)\n",
    "pyplot.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which of these functions have at least one root?\n",
    "* Which have more than one root?\n",
    "* Can we determine these properties merely by evaluating $f(x)$ for some values of $x$?\n",
    "\n",
    "## Bisection\n",
    "\n",
    "Bisection is a rootfinding technique that starts with an interval $[a,b]$ containing a root and does not require derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hasroot(f, a, b):\n",
    "    return f(a)[0]*f(b)[0] < 0\n",
    "    \n",
    "def bisect(f, a, b, history=None):\n",
    "    mid = (a + b)/2.\n",
    "    if b-a < 1e-5:\n",
    "        if history is None:\n",
    "            return mid\n",
    "        else:\n",
    "            return mid, numpy.array(history)\n",
    "    if history is not None:\n",
    "        history.append(mid)\n",
    "    if hasroot(f, a, mid):\n",
    "        return bisect(f, a, mid, history)\n",
    "    else:\n",
    "        return bisect(f, mid, b, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice that we need to define `hasroot` above.\n",
    "\n",
    "Let's try running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bisect(tests[0], -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.sqrt(2) - bisect(tests[0], 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get about 5 digits of accuracy.  Why?  How fast did we get there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, h = bisect(tests[0], 0, 2, history=[])\n",
    "x, h.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute and plot errors\n",
    "err = h - numpy.sqrt(2)\n",
    "pyplot.semilogy(numpy.abs(err), 'o')\n",
    "pyplot.semilogy(.5**numpy.arange(15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find any problems with this implementation?  List them below:\n",
    "\n",
    "* No error checking\n",
    "* \n",
    "\n",
    "Let's try running it on the rest of the test problem set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in tests:\n",
    "    print(f.__name__, bisect(f, -2, 2.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going wrong here?  How can we improve the implementation and what are fundamental limitations of the algorithm?\n",
    "\n",
    "### Convergence rate\n",
    "\n",
    "Let's quantitatively revisit the convergence rate.  A convergent rootfinding algorithm produces a sequence of approximations $x_i$ such that $$\\lim_{i \\to \\infty} x_i \\to x_*$$ where $f(x_*) = 0$.  For analysis, it is convenient to define the errors $e_i = x_i - x_*$. We say that an algorithm is **$q$-linearly convergent** if $$\\lim_{i \\to \\infty} |e_{i+1}| / |e_i| = \\rho < 1.$$  (The $q$ is for \"quotient\".)  A smaller convergence factor $\\rho$ represents faster convergence.  A slightly weaker condition ($r$-linear convergence or just **linear convergence**) is that\n",
    "$$ |e_i| \\le \\epsilon_i $$\n",
    "for all sufficiently large $i$ when $\\epsilon_i$ converges $q$-linearly to 0.\n",
    "\n",
    "Which criteria does the bisection method satisfy?  What is $\\rho$ for bisection?\n",
    "\n",
    "### Remarks on bisection\n",
    "\n",
    "* Specifying an interval is often inconvenient\n",
    "* An interval in which the function changes sign guarantees convergence (robustness)\n",
    "* No derivative information is required\n",
    "* Roots of even degree are problematic\n",
    "* A bound on the solution error is directly available\n",
    "* The convergence rate is modest -- one iteration per bit of accuracy\n",
    "\n",
    "## Newton-Raphson Method\n",
    "\n",
    "Much of numerical analysis reduces to [Taylor series](https://en.wikipedia.org/wiki/Taylor_series), the approximation\n",
    "$$ f(x) = f(x_0) + f'(x_0) (x-x_0) + f''(x_0) (x - x_0)^2 / 2 + \\dotsb $$\n",
    "centered on some reference point $x_0$.\n",
    "In numerical computation, it is exceedingly rare to look beyond the first-order approximation\n",
    "$$ \\tilde f_{x_0}(x) = f(x_0) + f'(x_0)(x - x_0) . $$\n",
    "Since $\\tilde f_{x_0}(x)$ is a linear function, we can explicitly compute the unique solution of $\\tilde f_{x_0}(x) = 0$ as\n",
    "$$ x = x_0 - f(x_0) / f'(x_0) . $$\n",
    "This is Newton's Method (aka Newton-Raphson or Newton-Raphson-Simpson) for finding the roots of differentiable functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton(f, x, verbose=False):\n",
    "    for i in range(100):\n",
    "        fx, dfx = f(x)\n",
    "        if verbose:\n",
    "            print(f.__name__, i, x, fx)\n",
    "        if numpy.abs(fx) < 1e-12:\n",
    "            return x, fx, i\n",
    "        try:\n",
    "            x -= fx / dfx\n",
    "        except ZeroDivisionError:\n",
    "            return x, numpy.NaN, i\n",
    "\n",
    "for f in tests:\n",
    "    print(f.__name__, newton(f, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Oops, how can we fix this?\n",
    "* This output is kinda hard to read, so let's make it cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in tests:\n",
    "    print(f.__name__, '{0:15.12f} {1:8.2e} {2:2d}'.format(\n",
    "        *newton(f, -0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Did we solve all of these equations?\n",
    "* How can the iteration break down?\n",
    "* Does choosing a different initial guess lead to different solutions?\n",
    "* How is this convergence test different from the one we used for bisection?\n",
    "* Is the convergence rate similar for all test equations?\n",
    "\n",
    "## Convergence of Newton-type algorithms\n",
    "\n",
    "We would like to know sharp conditions on when Newton-type algorithms converge, and if so, how fast.  This theory will build on that for a general _Fixed Point Iteration_ $x_{i+1} = g(x_i)$ where $g$ is a continuously differentiable function.  Suppose that there exists a fixed point $r = g(r)$.  By the mean value theorem, we have that\n",
    "$$ x_{i+1} - r = g(x_i) - g(r) = g'(c_i) (x_i - r) $$\n",
    "for some $c$ with $|c - r| < |x_i - r|$.\n",
    "In other words, $$|e_{i+1}| = |g'(c_i)| |e_i|$$, which converges to zero if $|g'(c_i)| < 1$.\n",
    "If $|g'(r)| < 1$ then for any $\\epsilon > 0$ there is a neighborhood of $r$ such that $|g'(c)| < |g'(r)| + \\epsilon$ for all $c$ in that neighborhood.\n",
    "Consequently, we have:\n",
    "\n",
    "#### Theorem: Linear Convergence of Fixed Point Iteration\n",
    "\n",
    "If $g$ is continuously differentiable, $r = g(r)$, and $|g'(r)| < 1$ then the fixed point iteration $x_{i+1} = g(x_i)$ is locally linearly convergent with rate $|g'(r)|$.\n",
    "\n",
    "#### Observations\n",
    "\n",
    "* A rootfinding problem $f(x) = 0$ can be converted to a fixed point problem $x = x - f(x) =: g(x)$ but there is no guarantee that $g'(r) = 1 - f'(r)$ will have magnitude less than 1.\n",
    "* Problem-specific algebraic manipulation can be used to make $|g'(r)|$ small.\n",
    "* $x = x - h(x)f(x)$ is also a valid formulation for any $h(x)$ bounded away from $0$.\n",
    "* Can we choose $h(x)$ such that $$ g'(x) = 1 - h'(x)f(x) - h(x)f'(x) = 0$$ when $f(x) = 0$?\n",
    "\n",
    "In other words,\n",
    "$$ x_{i+1} = x_i - f(x) \\underbrace{\\frac{1}{f'(x)}}_{h(x)} . $$\n",
    "\n",
    "* It turns out that Newton's method has _locally quadratic_ convergence to simple roots, $\\lim_{i \\to \\infty} |e_{i+1}|/|e_i|^2 < \\infty$.\n",
    "* \"The number of correct digits doubles each iteration.\"\n",
    "* Now that we know how to make a good guess accurate, the effort lies in getting a good guess.\n",
    "\n",
    "#### Culture: fast inverse square root\n",
    "\n",
    "The following code appeared literally (including comments) in the Quake III Arena source code (late 1990s).\n",
    "\n",
    "```C\n",
    "float Q_rsqrt( float number )\n",
    "{\n",
    "\tlong i;\n",
    "\tfloat x2, y;\n",
    "\tconst float threehalfs = 1.5F;\n",
    "\n",
    "\tx2 = number * 0.5F;\n",
    "\ty  = number;\n",
    "\ti  = * ( long * ) &y;                       // evil floating point bit level hacking\n",
    "\ti  = 0x5f3759df - ( i >> 1 );               // what the fuck? \n",
    "\ty  = * ( float * ) &i;\n",
    "    y  = y * ( threehalfs - ( x2 * y * y ) );   // 1st iteration\n",
    "//  y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed\n",
    "\n",
    "\treturn y;\n",
    "}\n",
    "```\n",
    "\n",
    "We now have [vector instructions](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=sqrt&expand=2989,1224,4470) for approximate inverse square root.\n",
    "More at https://en.wikipedia.org/wiki/Fast_inverse_square_root\n",
    "\n",
    "## Conditioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fquartic(x):\n",
    "    return (x - 0.9)**4, 4*(x - 0.9)**3\n",
    "\n",
    "newton(fquartic, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only get three digits correct despite a very small residual (and it takes many iterations to get there).\n",
    "Difficulty computing zeros of polynomials can also arise when all the roots are simple.  For example, the Wilkinson polynomial\n",
    "$$ \\prod_{i=1}^{20} (x - i) = \\sum_{i=0}^{20} a_i x^i $$\n",
    "has roots at each of the positive integers up to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wilkinson(n):\n",
    "    \"\"\"Compute Wilkinson polynomial of degree n in the\n",
    "    monomial basis p[0]*x^n + ... + p[n-1]*x + p[n].\n",
    "    The polynomial can be evaluated using numpy.polyval(p,x)\n",
    "    \"\"\"\n",
    "    p = numpy.array([1])\n",
    "    for i in range(1, n+1):\n",
    "        p = numpy.polymul(p, numpy.array([1, -i]))\n",
    "    return p\n",
    "\n",
    "wilkinson(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_wilkinson(n):\n",
    "    \"\"\"Plot the Wilkinson polynomial of degree n.\"\"\"\n",
    "    p = wilkinson(n)\n",
    "    #p[n//2] *= 1 + 1e-9\n",
    "    x = numpy.linspace(0, n+1, 100)\n",
    "    y = numpy.polyval(p, x)\n",
    "    pyplot.plot(x, y)\n",
    "    # pyplot.semilogy(x, numpy.abs(y))\n",
    "    \n",
    "plot_wilkinson(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The roots are extremely sensitive to perturbations of the coefficients $a_i$, as shown in this figure from Trefethen and Bau (1999).\n",
    "![Ill conditioning of roots of Wilkinson's polynomial](figures/TB-Wilkinson.png)\n",
    "\n",
    "Numerical difficulties in which \"correct\" algorithms produce unreliable solutions almost always stem from lack of *stability* and/or *ill conditioning*.\n",
    "\n",
    "### Absolute condition number\n",
    "Consider a function $f: X \\to Y$ and define the *absolute condition number*\n",
    "$$ \\hat\\kappa = \\lim_{\\delta \\to 0} \\max_{|\\delta x| < \\delta} \\frac{|f(x + \\delta x) - f(x)|}{|\\delta x|} = \\max_{\\delta x} \\frac{|\\delta f|}{|\\delta x|}. $$\n",
    "If $f$ is differentiable, then $\\hat\\kappa = |f'(x)|$.\n",
    "\n",
    "### Floating point arithmetic\n",
    "Floating point arithmetic $x \\circledast y := \\text{float}(x * y)$ is exact within a relative accuracy $\\epsilon_{\\text{machine}}$.  Formally,\n",
    "$$ x \\circledast y = (x * y) (1 + \\epsilon) $$\n",
    "for some $|\\epsilon| \\le \\epsilon_{\\text{machine}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "format((.2 - 1/3) + 2/15, '.20f')\n",
    "#format((.2 - 1/3) + (1/3 - 0.2), '.20f')\n",
    "#format((1 + 1e-12) - 1, '.20f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 1\n",
    "while 1 + eps > 1:\n",
    "    eps /= 2\n",
    "eps_machine = eps\n",
    "print('Machine epsilon = {}'.format(eps_machine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.log(1 + 1e-12) - numpy.log1p(1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(numpy.log(1 + 1e-12) - numpy.log1p(1e-12)) / numpy.log1p(1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = numpy.array([1,1e5,1e10,1e15])\n",
    "numpy.sin(numpy.pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.sin(x)**2 + numpy.cos(x)**2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[numpy.tan((3.14159+eps)/2) for eps in [1e-6,1e-8]], 1/numpy.cos(3.14159)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative condition number\n",
    "\n",
    "Given the relative nature of floating point arithmetic, it is more useful to discuss **relative condition number**,\n",
    "$$ \\kappa = \\max_{\\delta x} \\frac{|\\delta f|/|f|}{|\\delta x|/|x|}\n",
    "= \\max_{\\delta x} \\Big[ \\frac{|\\delta f|/|\\delta x|}{|f| / |x|} \\Big] $$\n",
    "or, if $f$ is differentiable,\n",
    "$$ \\kappa = \\max_{\\delta x} |f'(x)| \\frac{|x|}{|f|} . $$\n",
    "\n",
    "How does a condition number get big?\n",
    "\n",
    "#### Take-home message\n",
    "\n",
    "The relative accuracy of the best-case algorithm will not be reliably better than $\\epsilon_{\\text{machine}}$ times the condition number.\n",
    "$$ \\max_{\\delta x} \\frac{|\\delta f|}{|f|} \\ge \\kappa \\cdot \\epsilon_{\\text{machine}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical differentiation\n",
    "\n",
    "Suppose we want to apply Newton's method to a function that we know how to evaluate, but don't have code to differentiate.  This is often because it's difficult/error-prone to write or because the interface by which we call it does not support derivatives.  (Commercial packages often fall in this category.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff(f, x, epsilon=1e-5):\n",
    "    return (f(x + epsilon) - f(x)) / epsilon\n",
    "\n",
    "diff(numpy.sin, 0.7, 1e-8) - numpy.cos(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = .5\n",
    "diff(numpy.tan, x) - 1/numpy.cos(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 3.14/2\n",
    "[(eps, diff(numpy.tan, x, eps) - 1/numpy.cos(x)**2) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 1e4\n",
    "[(eps, diff(numpy.log, x, eps) - 1/x) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically choosing a suitable $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_wp(f, x, eps=1e-8):\n",
    "    \"\"\"Numerical derivative with Walker and Pernice (1998) choice of step\"\"\"\n",
    "    h = eps * (1 + abs(x))\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "x = 1\n",
    "[(eps, diff_wp(numpy.log, x, eps) - 1/x) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 1e-4\n",
    "[(eps, diff_wp(numpy.log, x, eps) - 1/x) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is imperfect, leaving some scaling responsibility to the user.\n",
    "It is the default in PETSc's \"matrix-free\" Newton-type solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of numerical differentiation\n",
    "\n",
    "##### Discretization error\n",
    "The `diff` and `diff_wp` functions use a \"forward difference\" formula: $\\tilde f'(x) := (f(x+h) - f(x))/h$.\n",
    "Using the Taylor expansion of $f(x+h)$, we compute the discretization error\n",
    "$$ \\begin{split} \\frac{f(x+h) - f(x)}{h} - f'(x) &= \\frac{f(x) + f'(x) h + f''(x) h^2/2 + O(h^3) - f(x)}{h} - f'(x) \\\\\n",
    "&= \\frac{f'(x) h + f''(x) h^2/2 + O(h^3)}{h} - f'(x) \\\\\n",
    "&= f''(x) h/2 + O(h^2)\n",
    "\\end{split} . $$\n",
    "\n",
    "This is the *discretization error* caused by choosing a finite (not infinitesimal) differencing parameter $h$, and the leading order term depends linearly on $h$.\n",
    "\n",
    "#### Rounding error\n",
    "We have an additional source of error, *rounding error*, which comes from not being able to compute $f(x)$ or $f(x+h)$ exactly, nor subtract them exactly.  Suppose that we can, however, compute these functions with a relative error on the order of $\\epsilon_{\\text{machine}}$.  This leads to\n",
    "$$ \\begin{split}\n",
    "\\tilde f(x) &= f(x)(1 + \\epsilon_1) \\\\\n",
    "\\tilde f(x \\oplus h) &= \\tilde f((x+h)(1 + \\epsilon_2)) \\\\\n",
    "&= f((x + h)(1 + \\epsilon_2))(1 + \\epsilon_3) \\\\\n",
    "&= [f(x+h) + f'(x+h)(x+h)\\epsilon_2 + O(\\epsilon_2^2)](1 + \\epsilon_3) \\\\\n",
    "&= f(x+h)(1 + \\epsilon_3) + f'(x+h)x\\epsilon_2 + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where each $\\epsilon_i$ is an independent relative error on the order of $\\epsilon_{\\text{machine}}$ and we have used a Taylor expansion at $x+h$ to approximate $f(x \\oplus h)$.\n",
    "We thus write the rounding error in the forward difference approximation as\n",
    "$$ \\begin{split}\n",
    "\\left\\lvert \\frac{\\tilde f(x+h) \\ominus \\tilde f(x)}{h} - \\frac{f(x+h) - f(x)}{h} \\right\\rvert &=\n",
    "  \\left\\lvert \\frac{f(x+h)(1 + \\epsilon_3) + f'(x+h)x\\epsilon_2 + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h) - f(x)(1 + \\epsilon_1) - f(x+h) + f(x)}{h} \\right\\rvert \\\\\n",
    "  &\\le \\frac{|f(x+h)\\epsilon_3| + |f'(x+h)x\\epsilon_2| + |f(x)\\epsilon_1| + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}}h)}{h} \\\\\n",
    "  &\\le \\frac{(2 \\max_{[x,x+h]} |f| + \\max_{[x,x+h]} |f' x| \\epsilon_{\\text{machine}} + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h)}{h} \\\\\n",
    "  &= (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h} + O(\\epsilon_{\\text{machine}}) \\\\\n",
    "\\end{split} $$\n",
    "where we have assumed that $h \\ge \\epsilon_{\\text{machine}}$.\n",
    "This error becomes large (relative to $f'$ -- we are concerned with relative error after all)\n",
    "* $f$ is large compared to $f'$\n",
    "* $x$ is large\n",
    "* $h$ is too small\n",
    "\n",
    "#### Total error and optimal $h$\n",
    "\n",
    "Suppose we would like to choose $h$ to minimize the combined discretization and rounding error,\n",
    "$$ h^* = \\arg\\min_h | f''(x) h/2 | + (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h} $$\n",
    "(dropping the higher order terms), which we can compute by differentiating with respect to $h$ and setting the result equal to zero\n",
    "$$ |f''|/2 - (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h^2} = 0 $$\n",
    "which can be rearranged as\n",
    "$$ h^* = \\sqrt{\\frac{4\\max|f| + 2\\max|f'x|}{|f''|}} \\sqrt{\\epsilon_{\\text{machine}}} .$$\n",
    "Of course this formula is of little use for computing $h$ because all this is to compute $f'$, which we obviously don't know yet, much less $f''$.\n",
    "However, it does have value:\n",
    "* It explains why `1e-8` (i.e., $\\sqrt{\\epsilon_{\\text{machine}}}$) was empirically found to be about optimal for well-behaved functions.\n",
    "* It explains why even for the best behaved functions, our best attainable accuracy with forward differencing is $\\sqrt{\\epsilon_{\\text{machine}}}$.\n",
    "* If we have some special knowledge about the class of functions we need to differentiate, we might have bounds on these quantities and thus an ability to use this formula to improve accuracy.  Alternatively, we could run a parameter sweep to empirically choose a suitable $h$, though we would have to re-tune in response to parameter changes in the class of functions.\n",
    "* If someone claims to have a simple and robust rule for computing $h$ then this formula tells us how to build a function that breaks their rule.  There are no silver bullets.\n",
    "* If our numerical differentiation routine produces a poor approximation for some function that we run into in the wild, this helps us explain what happened and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centered difference\n",
    "\n",
    "Instead of the forward difference approximation\n",
    "$$ \\frac{f(x+h) - f(x)}{h} $$\n",
    "we could use the centered difference formula,\n",
    "$$ \\frac{f(x+h) - f(x-h)}{2h} . $$\n",
    "(One way to derive this formula is to average a forward and backward difference.  We will learn a more general method later in the course when we do interpolation.)\n",
    "We can compute the discretization error by Taylor expansion,\n",
    "$$ \\frac{f(x) + f'(x)h + f''(x)h^2/2 + f'''(x)h^3/6 - f(x) + f'(x)h - f''(x)h^2/2 + f'''(x) h^3/6 + O(h^4)}{2h}\n",
    "= f'(x) + f'''(x)h^2/6 + O(h^3) $$\n",
    "showing that the leading error term is of order $h^2$, versus order $h$ for forward differences.\n",
    "A similar computation including rounding error will find that the optimal $h$ is now of order $\\sqrt[3]{\\epsilon_{\\text{machine}}}$ so the best attainable accuracy is $\\epsilon_{\\text{machine}}^{2/3}$.\n",
    "This accuracy improvement (versus $\\sqrt{\\epsilon_{\\text{machine}}}$) is significant, but we'll also see that it is twice as expensive when computing derivatives of multi-variate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability\n",
    "\n",
    "We use the notation $\\tilde f(x)$ to mean a numerical algorithm for approximating $f(x)$.  Additionally, $\\tilde x = x (1 + \\epsilon)$ is some \"good\" approximation of the exact input $x$.\n",
    "\n",
    "### (Forward) Stability\n",
    "**\"nearly the right answer to nearly the right question\"**\n",
    "$$ \\frac{\\lvert \\tilde f(x) - f(\\tilde x) \\rvert}{| f(\\tilde x) |} \\in O(\\epsilon_{\\text{machine}}) $$\n",
    "for some $\\tilde x$ that is close to $x$\n",
    "\n",
    "### Backward Stability\n",
    "**\"exactly the right answer to nearly the right question\"**\n",
    "$$ \\tilde f(x) = f(\\tilde x) $$\n",
    "for some $\\tilde x$ that is close to $x$\n",
    "\n",
    "* Every backward stable algorithm is stable.\n",
    "* Not every stable algorithm is backward stable.\n",
    "\n",
    "#### Example: $\\tilde f(x) = \\text{float}(x) + 1$\n",
    "The algorithm computes\n",
    "$$\\tilde f(x) = \\text{float}(x) \\oplus 1 = [x(1+\\epsilon_1) + 1](1 + \\epsilon_2) = (x + 1 + x\\epsilon_1)(1 + \\epsilon_2) $$\n",
    "and we can express any $\\tilde x = x(1 + \\epsilon_3)$.\n",
    "To see if if the algorithm is stable, we compute\n",
    "$$ \\frac{\\tilde f(x) - f(\\tilde x)}{|f(\\tilde x)|} = \\frac{(x + 1 + x\\epsilon_1)(1 + \\epsilon_2) - [x(1+ \\epsilon_3) + 1]}{\\tilde x + 1} = \\frac{(x + 1)\\epsilon_2 + x(\\epsilon_1 - \\epsilon_3) + O(\\epsilon^2)}{x + 1 + x\\epsilon_3} . $$\n",
    "If we can choose $\\epsilon_3$ to make this small, then the method will be (forward) stable, and if we can make this expression exactly zero, then we'll have backward stability.\n",
    "Trying for the latter, we solve for $\\epsilon_3$ by setting the numerator equal to zero,\n",
    "$$ \\epsilon_3 = \\frac{x + 1}{x}\\epsilon_2 + \\epsilon_1 + O(\\epsilon^2)/x $$\n",
    "which is small so long as $|x| \\gg 0$, but the first term blows up as $x \\to 0$.\n",
    "In other words, the fact that $\\epsilon_2$ can produce a large error relative to the input causes this algorithm to not be backward stable.\n",
    "In contrast, this $x\\to 0$ case is not a problem for forward stability because $\\epsilon_3 = \\epsilon_1$ yields error on the order of $\\epsilon_2$.\n",
    "\n",
    "#### Example: $\\tilde f(x,y) = \\text{float}(x) \\oplus \\text{float}(y)$\n",
    "\n",
    "Now we are interested in\n",
    "$$ \\frac{\\tilde f(x,y) - f(\\tilde x,\\tilde y)}{f(\\tilde x,\\tilde y)} $$\n",
    "and we can vary both $\\tilde x$ and $\\tilde y$.  If we choose $y=1$, then the ability to vary $\\tilde y$ is powerful enough to ensure backward stability.\n",
    "\n",
    "### Accuracy of backward stable algorithms (Theorem)\n",
    "\n",
    "A backward stable algorithm for computing $f(x)$ has relative accuracy\n",
    "$$ \\left\\lvert \\frac{\\tilde f(x) - f(x)}{f(x)} \\right\\rvert \\in O(\\kappa(f) \\epsilon_{\\text{machine}}) . $$\n",
    "This is a rewording of a statement made earlier -- backward stability is the best case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
